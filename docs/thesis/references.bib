@INPROCEEDINGS{CSCCFD,
	author={Alejandro Correa Bahnsen and Aleksandar Stojanovic and Djamila Aouada and Björn Ottersten},
	booktitle={2013 12th International Conference on Machine Learning and Applications},
	title={Cost Sensitive Credit Card Fraud Detection Using Bayes Minimum Risk},
	year={2013},
	volume={1},
	number={},
	pages={333-338},
	keywords={Bayes methods;credit transactions;fraud;learning (artificial intelligence);risk analysis;cost sensitive credit card fraud detection;Bayes minimum risk;card holders;machine learning;cost measure;real life transactional data;European card processing company;Databases;Credit cards;Radio frequency;Optimization;Training;Companies;Loss measurement;Credit card fraud detection;Bayesian decision theory;Cost sensitive classification},
	doi={10.1109/ICMLA.2013.68},
	ISSN={},
	month={Dec},
}
@inproceedings{ICCFD,
	author = {Correa Bahnsen, Alejandro and Stojanovic, Aleksandar and Aouada, Djamila and Ottersten, Björn},
	year = {2014},
	month = {04},
	pages = {},
	title = {Improving Credit Card Fraud Detection with Calibrated Probabilities},
	booktitle = {Proceedings of the 2014 SIAM International Conference on Data Mining. Pennsylvania, USA},
	doi = {10.1137/1.9781611973440.78}
}
@INPROCEEDINGS{EDCSLR,
	author={A. C. {Bahnsen} and D. {Aouada} and B. {Ottersten}},
	booktitle={2014 13th International Conference on Machine Learning and Applications},
	title={Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring},
	year={2014},
	volume={},
	number={},
	pages={263-269},
	keywords={financial management;matrix algebra;regression analysis;example-dependent cost-sensitive logistic regression;credit scoring;cost-sensitive classification;financial cost;lending business;example-dependent cost matrix;Radio frequency;Logistics;Training;Databases;Standards;Sensitivity;Cost function;Cost sensitive classification;Credit Scoring;Logistic Regression},
	doi={10.1109/ICMLA.2014.48},
	ISSN={null},
	month={Dec},}
@article{EDCSDT,
	author = {Alejandro Correa Bahnsen and Aouada, Djamila and Ottersten, Bj\"{o}rn},
	title = {Example-dependent Cost-sensitive Decision Trees},
	journal = {Expert Syst. Appl.},
	issue_date = {November 2015},
	volume = {42},
	number = {19},
	month = nov,
	year = {2015},
	issn = {0957-4174},
	pages = {6609--6619},
	numpages = {11},
	url = {http://dx.doi.org/10.1016/j.eswa.2015.04.042},
	doi = {10.1016/j.eswa.2015.04.042},
	acmid = {2799082},
	publisher = {Pergamon Press, Inc.},
	address = {Tarrytown, NY, USA},
	keywords = {Cost-sensitive classifier, Cost-sensitive learning, Credit scoring, Decision trees, Direct marketing, Fraud detection},
} 
@article{alej2015ensemble,
	author    = {Alejandro Correa Bahnsen and
	Djamila Aouada and
	Bj{\"{o}}rn E. Ottersten},
	title     = {Ensemble of Example-Dependent Cost-Sensitive Decision Trees},
	journal   = {CoRR},
	volume    = {abs/1505.04637},
	year      = {2015},
	url       = {http://arxiv.org/abs/1505.04637},
	archivePrefix = {arXiv},
	eprint    = {1505.04637},
	timestamp = {Mon, 13 Aug 2018 16:47:14 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/BahnsenAO15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{evaluation_metrics,
	author = {Powers, David},
	year = {2011},
	month = {01},
	pages = {2229-3981},
	title = {Evaluation: From precision, recall and F-measure to {ROC}, informedness, markedness \& correlation},
	volume = {2},
	journal = {J. Mach. Learn. Technol},
	doi = {10.9735/2229-3981}
}
@INPROCEEDINGS{balanced_accuracy,
	author={K. H. {Brodersen} and C. S. {Ong} and K. E. {Stephan} and J. M. {Buhmann}},
	booktitle={2010 20th International Conference on Pattern Recognition},
	title={The Balanced Accuracy and Its Posterior Distribution},
	year={2010},
	volume={},
	number={},
	pages={3121-3124},
	keywords={generalisation (artificial intelligence);pattern classification;performance evaluation;statistical distributions;balanced accuracy;posterior distribution;performance evaluation;classification algorithm;generalizability;Accuracy;Training;Inference algorithms;Probabilistic logic;Machine learning;Prediction algorithms;Approximation algorithms;classification performance;generalizability;bias;class imbalance},
	doi={10.1109/ICPR.2010.764},
	ISSN={1051-4651},
	month={Aug},}
@inproceedings{Sheng:2006:TMC:1597538.1597615,
	author = {Sheng, Victor S. and Ling, Charles X.},
	title = {Thresholding for Making Classifiers Cost-sensitive},
	booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1},
	series = {AAAI'06},
	year = {2006},
	isbn = {978-1-57735-281-5},
	location = {Boston, Massachusetts},
	pages = {476--481},
	numpages = {6},
	url = {http://dl.acm.org/citation.cfm?id=1597538.1597615},
	acmid = {1597615},
	publisher = {AAAI Press},
}
@inproceedings{CS-Learning,
	author = {Elkan, Charles},
	title = {The Foundations of Cost-sensitive Learning},
	booktitle = {Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2},
	series = {IJCAI'01},
	year = {2001},
	isbn = {1-55860-812-5, 978-1-558-60812-2},
	location = {Seattle, WA, USA},
	pages = {973--978},
	numpages = {6},
	url = {http://dl.acm.org/citation.cfm?id=1642194.1642224},
	acmid = {1642224},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
} 
@InProceedings{EMinML,
	author="Dietterich, Thomas G.",
	title="Ensemble Methods in Machine Learning",
	booktitle="Multiple Classifier Systems",
	year="2000",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1--15",
	abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
	isbn="978-3-540-45014-6"
}
@Article{Pasting,
	author="Breiman, Leo",
	title="Pasting Small Votes for Classification in Large Databases and On-Line",
	journal="Machine Learning",
	year="1999",
	month="Jul",
	day="01",
	volume="36",
	number="1",
	pages="85--103",
	abstract="Many databases have grown to the point where they cannot fit into the fast memory of even large memory machines, to say nothing of current workstations. If what we want to do is to use these data bases to construct predictions of various characteristics, then since the usual methods require that all data be held in fast memory, various work-arounds have to be used. This paper studies one such class of methods which give accuracy comparable to that which could have been obtained if all data could have been held in core and which are computationally fast. The procedure takes small pieces of the data, grows a predictor on each small piece and then pastes these predictors together. A version is given that scales up to terabyte data sets. The methods are also applicable to on-line learning.",
	issn="1573-0565",
	doi="10.1023/A:1007563306331",
	url="https://doi.org/10.1023/A:1007563306331"
}
@ARTICLE{Random_Subspace,
	author={ {Tin Kam Ho}},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={The random subspace method for constructing decision forests},
	year={1998},
	volume={20},
	number={8},
	pages={832-844},
	keywords={pattern classification;trees (mathematics);decision theory;random processes;learning (artificial intelligence);random subspace method;decision forests;decision trees;overfitting;maximum accuracy;decision tree based classifier;generalization accuracy;feature vector;classification accuracy;Decision trees;Classification tree analysis;Training data;Clustering algorithms;Tin;Stochastic systems;Binary trees;Support vector machines;Support vector machine classification},
	doi={10.1109/34.709601},
	ISSN={1939-3539},
	month={Aug},}
@InProceedings{Random_Patches,
	author="Louppe, Gilles
	and Geurts, Pierre",
	editor="Flach, Peter A.
	and De Bie, Tijl
	and Cristianini, Nello",
	title="Ensembles on Random Patches",
	booktitle="Machine Learning and Knowledge Discovery in Databases",
	year="2012",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="346--361",
	abstract="In this paper, we consider supervised learning under the assumption that the available memory is small compared to the dataset size. This general framework is relevant in the context of big data, distributed databases and embedded systems. We investigate a very simple, yet effective, ensemble framework that builds each individual model of the ensemble from a random patch of data obtained by drawing random subsets of both instances and features from the whole dataset. We carry out an extensive and systematic evaluation of this method on 29 datasets, using decision tree-based estimators. With respect to popular ensemble methods, these experiments show that the proposed method provides on par performance in terms of accuracy while simultaneously lowering the memory needs, and attains significantly better performance when memory is severely constrained.",
	isbn="978-3-642-33460-3"
}

@Article{Random_Forest,
	author="Breiman, Leo",
	title="Random Forests",
	journal="Machine Learning",
	year="2001",
	month="Oct",
	day="01",
	volume="45",
	number="1",
	pages="5--32",
	abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
	issn="1573-0565",
	doi="10.1023/A:1010933404324",
	url="https://doi.org/10.1023/A:1010933404324"
}

@inproceedings{xgboost,
	author = {Chen, Tianqi and Guestrin, Carlos},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	series = {KDD '16},
	year = {2016},
	isbn = {978-1-4503-4232-2},
	location = {San Francisco, California, USA},
	pages = {785--794},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	acmid = {2939785},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {large-scale machine learning},
}
@inproceedings{sklearn_api,
	author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
	Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
	Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
	and Jaques Grobler and Robert Layton and Jake VanderPlas and
	Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
	title     = {{API} design for machine learning software: experiences from the scikit-learn
	project},
	booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
	year      = {2013},
	pages = {108--122},
}
@misc{RMoser,
	author = {Roman Moser},
	title = {Fraud detection with cost-sensitive machine learning},
	year = 2019,
	howpublished = {\url{https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9}},
	urldate = {2020-01-06}
}
@misc{wiki:karta_platnicza,
	author = "{Wikipedia contributors}",
	title = "Karta płatnicza -- {W}ikipedia{,} Wolna encyklopedia",
	howpublished = {\url{https://pl.wikipedia.org/wiki/Karta_p\%C5\%82atnicza}},
	note = "[Online; dostęp 27-12-2019]"
}
@misc{wiki:payment_card,
	author = "{Wikipedia contributors}",
	title = "Payment card -- {W}ikipedia{,} Wolna encyklopedia",
	howpublished = {\url{https://en.wikipedia.org/wiki/Payment_card}},
	note = "[Online; dostęp 27-12-2019]"
}
@article{practitioner_perspective,
	author = {Dal Pozzolo, Andrea and Caelen, Olivier and Le Borgne, Yann-Aël and Waterschoot, Serge and Bontempi, Gianluca},
	year = {2014},
	month = {08},
	pages = {4915–4928},
	title = {Learned lessons in credit card fraud detection from a practitioner perspective},
	volume = {41},
	journal = {Expert Systems with Applications},
	doi = {10.1016/j.eswa.2014.02.026}
}
@misc{bahnsen_presentation,
	author = "Alejandro Correa Bahnsen",
	title = {Credit Card Fraud Detection: Why Theory Doesn't Adjust to Practice},
	howpublished = {\url{https://www.youtube.com/watch?v=YCNkxaVDiA0}},
	note = {SAS Analytics Conference, June 2, 2013, London, UK}
}
