@INPROCEEDINGS{CSCCFD,
	author={Alejandro Correa Bahnsen and Aleksandar Stojanovic and Djamila Aouada and Björn Ottersten},
	booktitle={2013 12th International Conference on Machine Learning and Applications},
	title={Cost Sensitive Credit Card Fraud Detection Using Bayes Minimum Risk},
	year={2013},
	volume={1},
	number={},
	pages={333-338},
	keywords={Bayes methods;credit transactions;fraud;learning (artificial intelligence);risk analysis;cost sensitive credit card fraud detection;Bayes minimum risk;card holders;machine learning;cost measure;real life transactional data;European card processing company;Databases;Credit cards;Radio frequency;Optimization;Training;Companies;Loss measurement;Credit card fraud detection;Bayesian decision theory;Cost sensitive classification},
	doi={10.1109/ICMLA.2013.68},
	ISSN={},
	month={Dec},
}
@inbook{ICCFD,
	author = {Alejandro Correa Bahnsen and Aleksandar Stojanovic and Djamila Aouada and Björn Ottersten},
	title = {Improving Credit Card Fraud Detection with Calibrated Probabilities},
	booktitle = {Proceedings of the 2014 SIAM International Conference on Data Mining},
	chapter = {},
	pages = {677-685},
	doi = {10.1137/1.9781611973440.78},
	URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611973440.78},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611973440.78}
}
@inproceedings{EDCSLR,
	author = {Alejandro Correa Bahnsen and Aouada, Djamila and Ottersten, Björn},
	year = {2014},
	month = {12},
	pages = {},
	title = {Example-Dependent Cost-Sensitive Logistic Regression for Credit Scoring},
	journal = {Proceedings - 2014 13th International Conference on Machine Learning and Applications, ICMLA 2014},
	doi = {10.1109/ICMLA.2014.48}
}
@article{EDCSDT,
	author = {Alejandro Correa Bahnsen and Aouada, Djamila and Ottersten, Bj\"{o}rn},
	title = {Example-dependent Cost-sensitive Decision Trees},
	journal = {Expert Syst. Appl.},
	issue_date = {November 2015},
	volume = {42},
	number = {19},
	month = nov,
	year = {2015},
	issn = {0957-4174},
	pages = {6609--6619},
	numpages = {11},
	url = {http://dx.doi.org/10.1016/j.eswa.2015.04.042},
	doi = {10.1016/j.eswa.2015.04.042},
	acmid = {2799082},
	publisher = {Pergamon Press, Inc.},
	address = {Tarrytown, NY, USA},
	keywords = {Cost-sensitive classifier, Cost-sensitive learning, Credit scoring, Decision trees, Direct marketing, Fraud detection},
} 
@misc{alej2015ensemble,
    title={Ensemble of Example-Dependent Cost-Sensitive Decision Trees},
    author={Alejandro Correa Bahnsen and Djamila Aouada and Bjorn Ottersten},
    year={2015},
    eprint={1505.04637},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{evaluation_metrics,
	author = {Powers, David and Ailab},
	year = {2011},
	month = {01},
	pages = {2229-3981},
	title = {Evaluation: From precision, recall and F-measure to ROC, informedness, markedness \& correlation},
	volume = {2},
	journal = {J. Mach. Learn. Technol},
	doi = {10.9735/2229-3981}
}
@INPROCEEDINGS{balanced_accuracy,
	author={K. H. {Brodersen} and C. S. {Ong} and K. E. {Stephan} and J. M. {Buhmann}},
	booktitle={2010 20th International Conference on Pattern Recognition},
	title={The Balanced Accuracy and Its Posterior Distribution},
	year={2010},
	volume={},
	number={},
	pages={3121-3124},
	keywords={generalisation (artificial intelligence);pattern classification;performance evaluation;statistical distributions;balanced accuracy;posterior distribution;performance evaluation;classification algorithm;generalizability;Accuracy;Training;Inference algorithms;Probabilistic logic;Machine learning;Prediction algorithms;Approximation algorithms;classification performance;generalizability;bias;class imbalance},
	doi={10.1109/ICPR.2010.764},
	ISSN={1051-4651},
	month={Aug},}
@inproceedings{Sheng:2006:TMC:1597538.1597615,
	author = {Sheng, Victor S. and Ling, Charles X.},
	title = {Thresholding for Making Classifiers Cost-sensitive},
	booktitle = {Proceedings of the 21st National Conference on Artificial Intelligence - Volume 1},
	series = {AAAI'06},
	year = {2006},
	isbn = {978-1-57735-281-5},
	location = {Boston, Massachusetts},
	pages = {476--481},
	numpages = {6},
	url = {http://dl.acm.org/citation.cfm?id=1597538.1597615},
	acmid = {1597615},
	publisher = {AAAI Press},
}
@inproceedings{CS-Learning,
	author = {Elkan, Charles},
	title = {The Foundations of Cost-sensitive Learning},
	booktitle = {Proceedings of the 17th International Joint Conference on Artificial Intelligence - Volume 2},
	series = {IJCAI'01},
	year = {2001},
	isbn = {1-55860-812-5, 978-1-558-60812-2},
	location = {Seattle, WA, USA},
	pages = {973--978},
	numpages = {6},
	url = {http://dl.acm.org/citation.cfm?id=1642194.1642224},
	acmid = {1642224},
	publisher = {Morgan Kaufmann Publishers Inc.},
	address = {San Francisco, CA, USA},
} 
@InProceedings{EMinML,
	author="Dietterich, Thomas G.",
	title="Ensemble Methods in Machine Learning",
	booktitle="Multiple Classifier Systems",
	year="2000",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="1--15",
	abstract="Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.",
	isbn="978-3-540-45014-6"
}
@Article{Pasting,
	author="Breiman, Leo",
	title="Pasting Small Votes for Classification in Large Databases and On-Line",
	journal="Machine Learning",
	year="1999",
	month="Jul",
	day="01",
	volume="36",
	number="1",
	pages="85--103",
	abstract="Many databases have grown to the point where they cannot fit into the fast memory of even large memory machines, to say nothing of current workstations. If what we want to do is to use these data bases to construct predictions of various characteristics, then since the usual methods require that all data be held in fast memory, various work-arounds have to be used. This paper studies one such class of methods which give accuracy comparable to that which could have been obtained if all data could have been held in core and which are computationally fast. The procedure takes small pieces of the data, grows a predictor on each small piece and then pastes these predictors together. A version is given that scales up to terabyte data sets. The methods are also applicable to on-line learning.",
	issn="1573-0565",
	doi="10.1023/A:1007563306331",
	url="https://doi.org/10.1023/A:1007563306331"
}
@ARTICLE{Random_Subspace,
	author={ {Tin Kam Ho}},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={The random subspace method for constructing decision forests},
	year={1998},
	volume={20},
	number={8},
	pages={832-844},
	keywords={pattern classification;trees (mathematics);decision theory;random processes;learning (artificial intelligence);random subspace method;decision forests;decision trees;overfitting;maximum accuracy;decision tree based classifier;generalization accuracy;feature vector;classification accuracy;Decision trees;Classification tree analysis;Training data;Clustering algorithms;Tin;Stochastic systems;Binary trees;Support vector machines;Support vector machine classification},
	doi={10.1109/34.709601},
	ISSN={1939-3539},
	month={Aug},}
@inproceedings{Random_Patches,
	author = {Louppe, Gilles and Geurts, Pierre},
	year = {2012},
	month = {09},
	pages = {346-361},
	title = {Ensembles on Random Patches},
	doi = {10.1007/978-3-642-33460-3_28}
}
@Article{Random_Forest,
	author="Breiman, Leo",
	title="Random Forests",
	journal="Machine Learning",
	year="2001",
	month="Oct",
	day="01",
	volume="45",
	number="1",
	pages="5--32",
	abstract="Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund {\&} R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.",
	issn="1573-0565",
	doi="10.1023/A:1010933404324",
	url="https://doi.org/10.1023/A:1010933404324"
}

@inproceedings{xgboost,
	author = {Chen, Tianqi and Guestrin, Carlos},
	title = {{XGBoost}: A Scalable Tree Boosting System},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	series = {KDD '16},
	year = {2016},
	isbn = {978-1-4503-4232-2},
	location = {San Francisco, California, USA},
	pages = {785--794},
	numpages = {10},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	acmid = {2939785},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {large-scale machine learning},
}
@inproceedings{sklearn_api,
	author    = {Lars Buitinck and Gilles Louppe and Mathieu Blondel and
	Fabian Pedregosa and Andreas Mueller and Olivier Grisel and
	Vlad Niculae and Peter Prettenhofer and Alexandre Gramfort
	and Jaques Grobler and Robert Layton and Jake VanderPlas and
	Arnaud Joly and Brian Holt and Ga{\"{e}}l Varoquaux},
	title     = {{API} design for machine learning software: experiences from the scikit-learn
	project},
	booktitle = {ECML PKDD Workshop: Languages for Data Mining and Machine Learning},
	year      = {2013},
	pages = {108--122},
}
@misc{RMoser,
	author = {Roman Moser},
	title = {Fraud detection with cost-sensitive machine learning},
	year = 2019,
	url = {https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9},
	urldate = {2020-01-06}
}
