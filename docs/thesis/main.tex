\documentclass{book}

\usepackage{amsmath}
\usepackage{listings}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{makecell}
\setcellgapes{5pt}

\usepackage[utf8]{inputenc}
\usepackage{polski}

\title{Detekcja oszustw z wykorzystaniem metod wrażliwych na koszt}
\author{Patryk Wielopolski}

\begin{document}
	
	\newcommand{\htx}{h_{\theta}(\boldsymbol{x_i})}
	\newcommand{\es}{\mathcal{S}}
	\newcommand{\iks}{\boldsymbol{x}}
	
	\newenvironment{talign}
	{\align}
	{\endalign}
	
	\newenvironment{talign*}
	{\csname align*\endcsname}
	{\endalign}

\maketitle

\chapter{Wstęp}
	- Rosnąca ilość transakcji kartami kredytowymi
	- Rosnący poziom fraudów na świecie
	
	- Flow transakcji oraz umiejscowienie systemu do detekcji fraudów
	- Aktualna sytuacja if-then + predicitve models
	
	If-then:
	- więcej niż 4 wypłaty z bankomatu w ciągu godziny
	- więcej niż 2 transakcje w ciągu 5 minut
	- transakcja w sklepie karta a następna zaraz w internecie
	
	Jeżeli więcej niż 1 reguła jest spełniona to odmowa transakcji. Pojawiają się problemy z niewykrywaniem nowych reguł oraz możliwe jest tworzenie tylko prostych reguł. Z drugiej strony są one proste do implementacji oraz interpretacji.
	

\chapter{Wprowadzenie teoretyczne}

W tej części zostaną wprowadzone wszelkie potrzebne miary skuteczności modeli oraz modele predykcyjne, które zostaną wykorzystane do przeprowadzenia eksperymentu. 

Modele predykcyjne zostały podzielone na dwie kategorie: standardowe oraz wrażliwe na koszt. Pierwsze z nich są powszechnie wykorzystywane w standardowych aplikacjach. Drugie z nich dzielą się na dwie podkategorie - \textit{Cost Sensitive Training} oraz \textit{Cost Sensitive Classification}.

\section{Miary skuteczności modeli}

\subsection{Macierz pomyłek}

W tej sekcji zdefiniujemy macierz pomyłek.
\begin{table}[h]
	\begin{center}
		\makegapedcells
		\begin{tabular}{cc|cc}
			\multicolumn{2}{c}{}     &   \multicolumn{2}{c}{Predykcja} \\
			&            &   Oszustwo &   Normalna     \\ 
			\cline{2-4}
			\multirow{2}{*}{\rotatebox[origin=c]{90}{Prawda}} & Oszustwo   & TP         & FN              \\
			& Normalna   & FP         & TN              \\ 
			\cline{2-4}
		\end{tabular}
	\end{center}
	\caption{Macierz pomyłek}
	\label{macierz-pomylek}
\end{table}


Na podstawie podanej macierzy pomyłek w tabeli \ref{macierz-pomylek} definiujemy następujące miary skuteczności modeli:

$$ \text{Skuteczność} = \frac{TP + TN}{TP + FP + FN + TN} $$
$$ \text{Precyzja} = \frac{TP}{TP + FP} $$
$$ \text{Czułość}= \frac{TP}{TP + FN} $$
$$ \text{F1 Score} = 2 \cdot \frac{\text{Precyzja} \cdot \text{Czułość}}{\text{Precyzja} + \text{Czułość}} $$

\subsection{Metryki wrażliwe na koszt}

Motywacją do powstania miar wrażliwych na koszt jest rzeczywista ewaluacja modeli. Podstawowe metryki nie uwzględniają różnicy w kosztach pomyłki dla fp i fn. Ponadto koszt fraudów znacząco różni się w zależności od przypadku.

\section{Standardowe modele}

\subsection{Regresja logistyczna}

	Regresja logistyczna należy do jednego z najbardziej podstawowych modeli statystycznych używanych do problemów klasyfikacyjnych. 
	\begin{equation}
		\hat{p} = P(y=1|\boldsymbol{x_i}) = h_{\theta}(\boldsymbol{x_i}) = g\left(\sum_{j=1}^k \theta^{(j)}x_i^{(j)} \right)
	\end{equation} 
	Standardowa funkcja straty przyjmuje następującą postać:
	$$ J(\theta) = \frac{1}{N} \sum_{i=1}^N J_i(\theta) $$
	gdzie funkcja $g(z)$ jest funkcją łączącą typu \textit{logit} i przyjmuje postać 
	$$ g(z) = \frac{1}{(1+e^{-z})} $$
	Natomiast 
	$$ J_i(\theta) = -y_i log(h_{\theta}(\boldsymbol{x_i})) - (1-y_i) log(1 - h_{\theta}(\boldsymbol{x_i})) $$
	 to standardowa entropia krzyżowa.

	% Analiza kosztów w regresji logistycznej

    Standard costs:
	$$
	J_i(\theta) \approx \left\{
	\begin{array}{rl}
	0, &\mbox{if $y_i \approx \htx$}, \\
	\infty, &\mbox{if $y_i \approx (1 - \htx)$}.
	\end{array}{}
	\right.
	$$
	
	Thus
	
	$$ C_{TP_i} = C_{TN_i} \approx 0 $$
	$$ C_{FP_i} = C_{FN_i} \approx \infty $$

	Wytrenowany, aby minimalizować błąd klasyfikacji, a ewaluowany na metryce kosztu.

\subsection{Drzewo decyzyjne}

	Drzewo klasyfikacyjne to przykład jednego z rodzaju drzew decyzyjnych, którego celem jest znalezienie najlepszego rozróżnienia pomiędzy klasami. W ogólności drzewo decyzyjne składa się z zestawu reguł.
	
	Drzewo składa się z węzłów, które są reprezentowane przez parę $(\iks^j, l^j)$, która oznacza podział zbioru obserwacji $\es$ na dwa zbiory: $\es^{l}$ oraz $\es^{r}$ względem wektora obserwacji $\iks$ oraz progu decyzyjnego $\l^j$ w następujący sposób:
	$$ \es^l = \{ \iks^* : \iks^* \in \es \land x^j_i \leq l^j \} \text{,} $$
	$$ \es^r = \{ \iks^* : \iks^* \in \es \land x^j_i > l^j \} \text{,} $$
	gdzie $\iks^j$ reprezentuje $j$-ty atrybut wektora $\iks$. Ponadto $\l^j$ jest wartością taką, że $\min{\iks^j} \leq l^j < \max{\iks^j}$. Ponadto warto zauważyć, że $\es^l \cup \es^r = \es$, co oznacza, że nasz podział rozdziela wektor obserwacji na dokładnie dwa rozłączne zbiory.
	Po znalezieniu optymalnego podziału zliczamy ilość pozytywnych próbek:
	$$ \es_1  = \{ \iks^* : \iks^* \in \es \land y_i = 1 \} \text{,} $$
	a następnie zliczamy procent pozytywnych próbek jako:
	$$ \pi_1 = \frac{|\es_1|}{|\es|} \text{.}$$
	Następnie dla każdego z liści jest obliczana wielkość jego zanieczyszczenia.
	\begin{itemize}
		\item Misclassification: $I_m(\pi_1) = 1 - \max(\pi_1, 1 - \pi_1)$
		\item Entropy: $I_e(\pi_1) = -\pi_1 \log(\pi_1) - (1 - \pi_1) \log (1 - \pi_1)$
		\item Gini: $I_g(\pi_1) = 2 \pi_1 (1 - \pi_1)$
	\end{itemize}{}
	Następnie dla każdego proponowanego podziału dla danej reguły $(\iks^j, l^j)$ liczony jest przyrost czystości w następujący sposób:
	$$ \text{Gain}(\iks^j, l^j) = I(\pi_1) - \frac{|\es^l|}{|\es|} I(\pi_1^l) - \frac{|\es^r|}{|\es|} I(\pi_1^r) \text{,}$$
	gdzie $I(\pi_1)$ może być dowolną z zaproponowanych miar zanieczyszczenia.
	Ostatecznie wybiera się ten podział, który maksymalizuje przyrost czystości, a następnie dzieli się zbiór $\es$ na podzbiory $\es^l$ i $\es^r$.
	W taki sposób jest pokonywany pojedynczy podział zbioru dla konkretnego węzła. Całe drzewo tworzy się poprzez kolejne podziały węzłów aż do momentu dotarcia przez algorytm do kryterium stopu.
	
	% Dopisać kryteria stopu?
	% Przycinanie drzew 

\subsection{Las losowy}
	Kolejne modele są przedstawicielami szerokiej klasy metod \textit{ensemble}, których celem jest złożenie predykcji wielu klasyfikatorów bazowych, aby poprawić generalizację pojedynczego estymatora. Wśród nich wyróżniamy dwie główne kategorie. Pierwsza z nich to metody uśredniania, które polegają na zbudowaniu wielu niezależnych klasyfikatorów, a następnie uśrednianie wyników predykcji. Przedstawicielem tej kategorii jest las losowy. Natomiast druga polega na iteracyjnym budowaniu kolejnych modeli, które próbują zredukować obciążenie poprzednika. Bardzo powszechnie znanym reprezentantem jest algorytm XGBoost, którym zajmiemy się w następnym podrozdziale.
		
	Metody \textit{ensemble} wykorzystują metody próbkowania w celu utworzenia różnych klasyfikatorów bazowych, aby następnie dokonać ostatecznej predykcji. Metody te są używane, aby zredukować wariancję klasyfikatora bazowego (zazwyczaj drzewa decyzyjnego) poprzez losowe dobieranie próbki i/lub zmiennych, na których model będzie uczony. W wielu przypadkach stworzenie modelu opartego o \textit{bagging} jest znacznie prostsze, ponieważ wymaga jedynie zmiany próbkowania danych bez naruszania modelu bazowego, natomiast metody typu \text{boosting} wymagają zmiany całego algorytmu. Z licznych obserwacji wynika, że pierwsza z metod dużo lepiej radzi sobie mają jako bazowe klasyfikatory złożone modele, w przeciwieństwie do drugiej, która zazwyczaj najlepiej performuje wykorzystując proste modele (np. płytkie drzewa decyzyjne, tzn. o małej głębokości).
	
	% TODO: Złączyć oba opisy w jeden.
	
	Przykładowe metody losowania próbek do modeli bazowych:
	\begin{itemize}
		\item \textit{Pasting} - losowanie obserwacji bez powtórzeń
		\item \textit{Bagging} - losowanie obserwacji z powtórzeniami
		\item \textit{Random Subspaces} - losowanie podzbioru zmiennych
		\item \textit{Random Patches} - losowanie podzbioru zmiennych oraz obserwacji 
	\end{itemize}
	
	
	W przypadku lasu losowego proces losowania próbek jest podzielony na dwie fazy. Pierwsza z nich polega na próbkowaniu z powtórzeniami obserwacji ze zbioru treningowego dla każdego z osobnych estymatorów bazowych. Następnie podczas fazy tworzenia kolejnych węzłów w drzewach wybierany jest losowy podzbiór zmiennych, które mogą być w tym kroku wykorzystane. 
	
	Celem tych dwóch różnych źródeł losowości jest redukcja wariancji lasu. Pojedyncze drzewa mają tendencję do zbytniego dopasowywania się do danych, zatem losowanie poszczególnych zmiennych w każdym kolejnym węźle pomaga to zredukować. Natomiast losowanie różnych próbek do każdego z klasyfikatorów pozwala na stworzenie lekko odmiennych modeli bazowych.

\subsection{XGBoost}
	Tak jak wspomnieliśmy w poprzednim rozdziale algorytm XGBoost jest przykładem klasyfikatora, który w iteracyjny sposób tworzy kolejne bazowe klasyfikatory.
	

\section{Cost Dependent Classification}
	
	\subsection{Optymalizacja progu}
	
	
	
	\subsection{Bayesian Minimum Risk}
	
	Risk associated with predictions:
	
	$$ R(p_f|x) = L(p_f|y_f)P(p_f|x) + L(p_f|y_l)P(y_l|x) $$
	$$ R(p_l|x) = L(p_l|y_l)P(p_l|x) + L(p_l|y_f)P(y_f|x) $$
	
	Classification threshold:
	
	$$ R(p_f|x) \leq R(p_l|x)$$
	
	Where:
	
	\begin{itemize}
		\item $P(p_f|x)$, $P(p_l|x)$ - estimated probability of fraud/legimate transaction
		\item $L(p_{i}|y_{j})$ and $i,j \in \{l,f\}$ - loss function
	\end{itemize}{}
	Exact formula:
	
	$$ P(p_f|x) \ge \frac{L(p_f|y_l) - L(p_l|y_l)}{L(p_l|y_f) - L(p_f|y_f) - L(p_l|y_l) + L(p_f|y_l)}$$
	
	After reformulation:
	
	$$ p \ge \frac{C_{FP} - C_{TN}}{C_{FN} - C_{TP} - C_{TN} + C_{FP}}$$

	\section{Cost Sensitive Training}
	
		Pierwszą podgrupą metod wrażliwych na koszt jest \textit{Cost Sensitive Trainig}. Są to metody, które 
	
	\subsection{Regresja logistyczna wrażliwa na koszt}
	
	    Actual costs:
	
		$$
		J^c_i(\theta)=\left\{
		\begin{array}{rl}
		C_{TP_i}, &\mbox{if $y_i = 1$ and $\htx \approx 1$}, \\
		C_{TN_i}, &\mbox{if $y_i = 0$ and $\htx \approx 0$}, \\
		C_{FP_i}, &\mbox{if $y_i = 0$ and $\htx \approx 1$}, \\
		C_{FN_i}, &\mbox{if $y_i = 1$ and $\htx \approx 0$}.
		\end{array}
		\right.
		$$
		
		Cost sensitive loss function:
		\begin{talign*}
			J^c(\theta) &= \frac{1}{N} \sum_{i=1}^{N} \bigg( y_i \Big( \htx C_{TP_i} + (1 - \htx)C_{FN_i} \Big) \\
			&+ (1-y_i) \Big( \htx C_{FP_i} + (1 - \htx)C_{TN_i} \Big) \bigg)
		\end{talign*}
	
	\subsection{Drzewo decyzyjne wrażliwe na koszt}
	
	
	
		
		Cost Sensitive impurity measure:
		$ I_c(\es) = min \left\{ Cost(f_0(\es)), Cost(f_1(\es)) \right\}$
		
		$$ f(\es) =  \left\{
			\begin{array}{rl}
				0, &\mbox{jeżeli $\text{Cost}(f_0(\es)) \leq \text{Cost}(f_1(\es))$}, \\
				1, &\mbox{w przeciwnym wypadku}.
			\end{array}{}
		\right.
		$$
	
	

\chapter{Eksperyment}

Celem eksperymentu jest zbadanie jaki wpływ mają na miarę F1 oraz oszczędności mają poszczególne algorytmy.

Do eksperymentu zostanie wykorzystany zbiór danych Credit Card Fraud Detection zawierający 284,807 transakcji w tym zaledwie 492 oszustw. Tabela składa się z 30 kolumn, w tym 28 z nich są to nienazwane, zanonimizowane zmienne, które były wcześniej poddane transformacji PCA (\textit{ang. Principal Component Analysis}), dodatkowo posiadamy informacje dot. czasu transakcji oraz kwoty. 

Mimo tego, że dane są zanonimizowane można mieć pewne intuicje na temat tego jakie zmienne zostały użyte z zbiorze danych. 
Raw data:
- ID klienta, data transakcji, kwota, lokalizacja, typ transakcji (internet, płatność w sklepie, wypłata z bankomatu), rodzaj transakcji (linie lotnicze, hotel, wypożyczalnia samochodów), fraud, wiek klienta, kraj zamieszkania, kod pocztowy, typ karty
Na podstawie ref zmienne, które wykorzystuje się do tego typu problemów to:
- agregaty czasowe, np. ilość transakcji dla tego samego klienta w ciągu ostatnich 6 godzin, suma transakcji z ostatnich 7 dni itp.
W ogólności są to agregaty klient/karta kredytowa x typ transakcji/sklep/kategorai sklepu/kraj x ostatnie godziny/dni/tygodnie/miesiące x ilość/średnia/suma


Rozkład kwoty...

Eksperyment został przeprowadzony w następujący sposób:
50-krotnie dzielimy zbiór danych w proporcjach 50:17:33 na zbiór treningowy, walidacyjny oraz testowy. Następnie uczymy wszystkie modele na zbiorze treningowym. Dla modelu XGBoost wykorzystujemy zbiór walidacyjny do procesu wczesnego zatrzymywania (\textit{ang. Early stopping}), natomiast dla modeli BMR oraz TO korzystamy z tego zbioru jako zbiór treningowy. Następnie dla wszystkich modeli dokonujemy predykcyji na zbiorze testowym i mierzymy skuteczność typowań.

\chapter{Rezultaty}

\chapter{Podsumowanie}

% Referencje
% https://towardsdatascience.com/fraud-detection-with-cost-sensitive-machine-learning-24b8760d35d9
% https://www.youtube.com/watch?v=YCNkxaVDiA0
% https://www.slideshare.net/albahnsen/correa-bahnsen-alejandroanalytics2013slideshare - flow of transactions
% https://scikit-learn.org/stable/modules/ensemble.html

% TODO:
% Resampling to LR

\end{document}